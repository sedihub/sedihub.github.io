<!doctype html>
<html lang="en-us">
<head>
    <title>Reinforcement Learning Cheat Sheet - Side Notes</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://sedihub.github.io/css/bootstrap.css">
    <link rel="stylesheet" href="https://sedihub.github.io/css/custom.css">
</head>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<body>

    <main id="main">
        <div class="container">
            <div class="row justify-content-center text-center my-5">
                <div class="col-lg-10">
                    
                    <h1 class="mb-4 text-center">Reinforcement Learning Cheat Sheet</h1>
                    <p class="small mb-5 text-center"><span class="text-uppercase">July 5, 2020</span></p>
                </div>
                <div class="col-9 text-right">
                    <a href="https://sedihub.github.io" class="text-secondary">Home</a>
                </div>
            </div>
        </div>
        <div class="bg-skew bg-skew-light">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="col-lg-8">
                        <article class="pb-2">
                            <p>Here are my notes on reinforcement learning (RL). The goal is to use these as a quick reference. Hopefully, others would find it useful, too!</p>
<hr>
<h3 id="fundamental-notions">Fundamental Notions</h3>
<p>Main ingredients in RL:</p>
<ul>
<li>Environment</li>
<li>Reward Signal</li>
<li>Agent
<ul>
<li>Agent State</li>
<li>Value Function</li>
<li>Policy: A mapping from states to actions.</li>
<li>Model</li>
</ul>
</li>
</ul>
<p>The dilemma of &ldquo;Exploitation vs. Exploration&rdquo;&hellip;</p>
<p>All components are function in RL.</p>
<p>The domain of interest is &ldquo;Deep Reinforcement Learning&rdquo;.</p>
<hr>
<h3 id="mathematical-formulation">Mathematical Formulation</h3>
<p>Reward, $R_{t}$, is a scalar feedback signal. The agent&rsquo;s job is to maximize the cumulative reward, $G$:
$$
G_t = R_{t+1} + R_{t+2} + \cdots
$$
This cumulative reward defines the agent&rsquo;s goal and is referred to as &ldquo;return&rdquo;.</p>
<p><strong>Reward Hypothesis:</strong> &ldquo;Any goal can be formalized as the outcome of maximizing a cumulative reward.&rdquo;</p>
<p><strong>Value:</strong> We call the expected cumulative reward from a state $s$ the value:
$$
v(s) = \mathbb{E} \left[ G_t | S_t= s \right]
$$
(Recall, $G_{t} = R_{t+1} + G_{t+1}$)</p>
<p>It is possible to condition the value on action:
$$
q(s, a) = \mathbb{E} \left[ G_t | S_t= s, A_t=a \right]
$$</p>
<p><strong>History:</strong> The sequence of actions, rewards and observations:
$$
\mathcal{H} _{t} = O _{0}, A _{0}, R _{1}, O _{1}, A _{1}, R _{3}, \cdots, O _{t-1}, A _{t-1}, R _{t}, O _{t}
$$</p>
<p><strong>Agent&rsquo;s and Environment&rsquo;s States:</strong> Generally, the agent and the environment have separate internal states. The agent often is not aware of the environment&rsquo;s state. Agent&rsquo;s state is a function of history. The agent&rsquo;s state is typically much smaller than the environment&rsquo;s state.</p>
<p><strong>Markov Decision Process (MDP):</strong> A decision process is Markov if &ldquo;the future is independent of past given the present&rdquo;. Mathematically, this can be expressed as:
$$
P(r,s | S _t, A _t) = P(r,s|\mathcal{H} _{t}, A _{t})
$$</p>
<p>This holds when we have full observability, i.e., the agent is fully aware of the environment&rsquo;s state. When the agent is only partially aware of the environment&rsquo;s state the observation is no longer Markov. Formally, this is referred to as &ldquo;partially observable Markov decision process (POMDP)&rdquo;.</p>
<p><strong>Bellman  Equation:</strong> (Bellman 1957)
$$
v_{\pi}(s) = \mathbb{E}\left[ G_t | S_t, \pi \right]
$$
here, $a ~ \pi(s)$. Similarly, the optimal (highest possible) value, which does not depend on any policy, is:
$$
v_*(s) = \max_{a} \mathbb{E}\left[ R_{t+1} + \gamma v_{*}(S_t+1) | S_t=s, A_t=a \pi \right]
$$
where $\pi$ is the policy. Often we use a discount factor, $\gamma \in [0, 1]$ for return:
$$
\mathcal{G}_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots$
$$</p>

                            <p class="mt-5 text-center">
                                
                                
                            </p>
                        </article>
                        <div class="row pt-5 pb-5">
                            <div class="col-6 text-left">
                            
                              <a class="text-reset" href="https://sedihub.github.io/posts/restart/">&larr; Restart</a>
                            
                            </div>
                            <div class="col-6 text-right">
                            
                              <a class="text-reset" href="https://sedihub.github.io/posts/bessels_correction/">Bessel&#39;s Correction &rarr;</a>
                            
                            </div>
                        </div>                                                
                    </div>
                </div>
            </div>
        </div>
    </main>
<footer class="site-footer mt-5">
  <div class="container">
      <div class="row justify-content-md-between">
          <div class="col-sm-12 col-md-4 mb-4">
              <h2 class="h5 mb-3">Side Notes</h2>
              <p>A Collection of my project notes.</p>
          </div>
          <div class="col-4 col-md-2 mb-4">
              <h2 class="h5 mb-3">Menu</h2>
              <ul class="nav flex-column">
                  <li class="mb-1"><a href="https://sedihub.github.io" class="text-secondary">Home</a></li>
                  
              </ul>
          </div>
      </div>

      <hr/>


  </div>
</footer>
</body>
</html>

